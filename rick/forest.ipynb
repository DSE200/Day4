{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forests\n",
    "\n",
    "A collection of decision trees that work together to make better predictions than any single tree could make on its own.\n",
    "\n",
    "### Advantage\n",
    "A single decision tree can overfit—it memorizes the training data too well and doesn't generalize to new situations. A random forest solves this by building many slightly different decision trees, each one learning from a slightly different view of your data, then averaging their predictions together.\n",
    "\n",
    "### How it works:\n",
    "Each tree in the forest is trained on a random sample of your data (with replacement—meaning some rows might be used multiple times, others skipped). Additionally, at each node when the tree decides which feature to split on, it only considers a random subset of features to choose from. These two random choices—different data samples and different feature choices—mean each tree ends up being unique and makes different mistakes.\n",
    "\n",
    "When you want to make a prediction on new data, all the trees vote. For classification, the class that most trees predict wins. For regression, you take the average of all the predictions. This \"wisdom of the crowd\" effect means the errors from individual trees cancel each other out."
   ],
   "metadata": {
    "id": "jwfUFkmdN-oK"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye8dvnDffAP0"
   },
   "source": [
    "# Random Forests: Why Ensembles Beat Single Trees\n",
    "## Customer Churn Prediction\n",
    "\n",
    "You just saw how decision trees work. They're interpretable and powerful, but they have a problem: **instability**.\n",
    "\n",
    "Small changes in data → very different trees. This is where **Random Forests** come in.\n",
    "\n",
    "**Today:** Understand why ensembles reduce variance and produce better predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bULdEmo4fAP2"
   },
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "We'll use the same libraries as before, plus RandomForestClassifier from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EqSuEG2OfAP3"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make plots look nice\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vd0X6kZ8fAP3"
   },
   "source": [
    "## Part 1: Load & Prepare Data\n",
    "\n",
    "Same churn dataset as before. We'll use it to compare single trees vs. ensembles."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fGwIzlS8fAP3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762404352951,
     "user_tz": 480,
     "elapsed": 333,
     "user": {
      "displayName": "Richard Gessner",
      "userId": "04866485102218961500"
     }
    },
    "outputId": "c062349b-7969-4fd9-a420-e01b38e1e4de",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Load the telecom churn dataset from GitHub\n",
    "url = 'https://raw.githubusercontent.com/TUHHStartupEngineers/dat_sci_ss20/master/13/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {(df['Churn'] == 'Yes').mean():.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH12d1iZfAP4"
   },
   "source": [
    "### Prepare Data\n",
    "\n",
    "Encode categorical features, convert target to binary, split train/test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NBEXi9UmfAP4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762404355603,
     "user_tz": 480,
     "elapsed": 223,
     "user": {
      "displayName": "Richard Gessner",
      "userId": "04866485102218961500"
     }
    },
    "outputId": "32b5d4f1-adf5-42a0-f730-436680932c34",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Clean and prepare data\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.drop(columns=['customerID'])\n",
    "\n",
    "# Target: convert Yes/No to 1/0\n",
    "y = (df_clean['Churn'] == 'Yes').astype(int)\n",
    "X = df_clean.drop(columns=['Churn'])\n",
    "\n",
    "# Encode categorical features\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Fix TotalCharges column\n",
    "X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOwpDxy9fAP4"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Why Ensembles?\n",
    "\n",
    "### The Problem with Single Trees\n",
    "\n",
    "Decision trees have high **variance**: small changes in data → very different trees → unstable predictions.\n",
    "\n",
    "**The Ensemble Idea:** Train *many* trees on slightly different versions of the data, then average their predictions. This reduces variance and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWD2Bp1DfAP4"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Train & Compare\n",
    "\n",
    "Let's train:\n",
    "- 1 decision tree (to establish a baseline)\n",
    "- Random forests with 50, 100, and 500 trees\n",
    "\n",
    "Then compare test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "veBD18iPfAP4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762404363758,
     "user_tz": 480,
     "elapsed": 4954,
     "user": {
      "displayName": "Richard Gessner",
      "userId": "04866485102218961500"
     }
    },
    "outputId": "57d3f4af-8ec1-4092-b5d8-25e921b499d2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Train a single decision tree for comparison\n",
    "print(\"Training single decision tree...\")\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_acc = tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Single Tree Test Accuracy: {tree_acc:.3f}\")\n",
    "\n",
    "# Train random forests with different numbers of trees\n",
    "results = {'model': ['Single Tree'], 'n_estimators': [1], 'test_accuracy': [tree_acc]}\n",
    "\n",
    "for n_trees in [50, 100, 500]:\n",
    "    print(f\"\\nTraining Random Forest with {n_trees} trees...\")\n",
    "    forest = RandomForestClassifier(n_estimators=n_trees, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    forest.fit(X_train, y_train)\n",
    "    forest_acc = forest.score(X_test, y_test)\n",
    "    print(f\"  Test Accuracy: {forest_acc:.3f}\")\n",
    "\n",
    "    results['model'].append(f'Forest ({n_trees})')\n",
    "    results['n_estimators'].append(n_trees)\n",
    "    results['test_accuracy'].append(forest_acc)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON:\")\n",
    "print(results_df.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ax-oHZsfAP4"
   },
   "source": [
    "### Visualize the Comparison\n",
    "\n",
    "How much does ensemble size matter?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3-7eJL_MfAP5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762404368100,
     "user_tz": 480,
     "elapsed": 1393,
     "user": {
      "displayName": "Richard Gessner",
      "userId": "04866485102218961500"
     }
    },
    "outputId": "938952e2-c24f-4ab9-a21a-01f16ef1f8fb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    }
   },
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red'] + ['green'] * (len(results_df) - 1)\n",
    "bars = plt.bar(range(len(results_df)), results_df['test_accuracy'], color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xticks(range(len(results_df)), results_df['model'], rotation=15, ha='right')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Single Tree vs Random Forests: Accuracy Comparison')\n",
    "plt.ylim([0.7, 0.85])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Forest beats single tree. More trees help, then plateau.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me9rtxzCfAP5"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: The Accuracy Plateau\n",
    "\n",
    "Adding more trees improves accuracy at first, but then it plateaus. Let's see this more clearly by training forests with many different sizes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r2Y65G3mfAP5"
   },
   "source": [
    "# Train forests with different numbers of trees to see the plateau\n",
    "print(\"Training forests with increasing numbers of trees...\")\n",
    "n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "accuracies = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    forest = RandomForestClassifier(n_estimators=n, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    forest.fit(X_train, y_train)\n",
    "    acc = forest.score(X_test, y_test)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"  n_estimators={n:3d} → accuracy={acc:.4f}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnBbk5hXfAP5"
   },
   "source": [
    "### Plot the Plateau\n",
    "\n",
    "This is the key insight: Why does accuracy plateau? When does it stop improving?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1c5OnlYcfAP5"
   },
   "source": [
    "# Plot accuracy vs number of trees\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_estimators_range, accuracies, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xscale('log')  # Log scale shows the diminishing returns more clearly\n",
    "plt.xlabel('Number of Trees (log scale)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Random Forest Accuracy vs Number of Trees')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.axhline(y=max(accuracies), color='r', linestyle='--', alpha=0.5, label='Best accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAccuracy plateau:\")\n",
    "print(f\"  With 1 tree: {accuracies[0]:.4f}\")\n",
    "print(f\"  With 10 trees: {accuracies[2]:.4f} (improvement: {(accuracies[2]-accuracies[0])*100:.2f}%)\")\n",
    "print(f\"  With 100 trees: {accuracies[5]:.4f} (improvement from 10: {(accuracies[5]-accuracies[2])*100:.2f}%)\")\n",
    "print(f\"  With 500 trees: {accuracies[7]:.4f} (improvement from 100: {(accuracies[7]-accuracies[5])*100:.2f}%)\")\n",
    "print(\"\\n→ Diminishing returns: More trees help, but the gain gets smaller.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLkvTN4QfAP5"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 5: Feature Importance Comparison\n",
    "\n",
    "Do single trees and forests rely on the same features?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6KrwM3MWfAP5"
   },
   "source": [
    "# Train a forest for feature importance comparison\n",
    "forest_best = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1)\n",
    "forest_best.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance from both\n",
    "tree_importance = tree.feature_importances_\n",
    "forest_importance = forest_best.feature_importances_\n",
    "\n",
    "# Create comparison dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'single_tree': tree_importance,\n",
    "    'forest': forest_importance\n",
    "}).sort_values('forest', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Comparison (Top 10):\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSnEU4HTfAP5"
   },
   "source": [
    "### Visualize Feature Importance Comparison\n",
    "\n",
    "Do they rank features the same way?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4GYHcQZWfAP5"
   },
   "source": [
    "# Plot side-by-side comparison\n",
    "top_features = importance_df.head(10)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Single tree\n",
    "ax1.barh(range(len(top_features)), top_features['single_tree'], color='coral', alpha=0.7)\n",
    "ax1.set_yticks(range(len(top_features)))\n",
    "ax1.set_yticklabels(top_features['feature'])\n",
    "ax1.set_xlabel('Importance')\n",
    "ax1.set_title('Single Decision Tree\\nTop 10 Features')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Forest\n",
    "ax2.barh(range(len(top_features)), top_features['forest'], color='steelblue', alpha=0.7)\n",
    "ax2.set_yticks(range(len(top_features)))\n",
    "ax2.set_yticklabels(top_features['feature'])\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Random Forest (100 trees)\\nTop 10 Features')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Do they rank the top features the same way?\")\n",
    "print(f\"Single tree top feature: {importance_df.iloc[0]['feature']}\")\n",
    "print(f\"Forest top feature: {importance_df.iloc[0]['feature']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7tpl7BifAP5"
   },
   "source": [
    "---\n",
    "\n",
    "## YOUR TURN: Pairs Practice (20 minutes)\n",
    "\n",
    "Now you build your own ensemble and experiment with parameters.\n",
    "\n",
    "### Task 1: Train & Compare (10 min)\n",
    "\n",
    "- Train a single tree (max_depth=5) on the churn data\n",
    "- Train a random forest with 100 trees\n",
    "- Compare test accuracies\n",
    "- **Does the forest beat the tree?**\n",
    "\n",
    "### Task 2: See the Plateau (5 min)\n",
    "\n",
    "- Train forests with n_estimators = 10, 50, 100, 200\n",
    "- Plot accuracy for each\n",
    "- **At what point do you see diminishing returns?**\n",
    "\n",
    "### Task 3: Reflect (5 min)\n",
    "\n",
    "- If you had to choose: single tree or forest? Why?\n",
    "- When would you *not* use a forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFtzw5lNfAP5"
   },
   "source": [
    "---\n",
    "\n",
    "## Student Practice: Code Along Below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mAWchq6GfAP5"
   },
   "source": [
    "# TASK 1: Train your own single tree and forest\n",
    "# TODO: Train a DecisionTreeClassifier and a RandomForestClassifier\n",
    "# Hint: Use max_depth=5 for the tree, n_estimators=100 for the forest\n",
    "\n",
    "my_tree = None  # REPLACE THIS\n",
    "my_forest = None  # REPLACE THIS\n",
    "\n",
    "if my_tree and my_forest:\n",
    "    tree_acc = my_tree.score(X_test, y_test)\n",
    "    forest_acc = my_forest.score(X_test, y_test)\n",
    "\n",
    "    print(f\"Your Single Tree Accuracy: {tree_acc:.4f}\")\n",
    "    print(f\"Your Forest Accuracy: {forest_acc:.4f}\")\n",
    "    print(f\"\\nImprovement: {(forest_acc - tree_acc)*100:.2f}%\")\n",
    "    print(f\"Forest wins: {forest_acc > tree_acc}\")\n",
    "else:\n",
    "    print(\"TODO: Train your models in this cell\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qnkuzywmfAP5"
   },
   "source": [
    "# TASK 2: Train forests with different n_estimators and see the plateau\n",
    "# TODO: Train forests with 10, 50, 100, 200 trees and record accuracy\n",
    "\n",
    "my_n_estimators = [10, 50, 100, 200]\n",
    "my_accuracies = []\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# for n in my_n_estimators:\n",
    "#     forest = RandomForestClassifier(n_estimators=n, max_depth=5, random_state=42)\n",
    "#     forest.fit(X_train, y_train)\n",
    "#     acc = forest.score(X_test, y_test)\n",
    "#     my_accuracies.append(acc)\n",
    "\n",
    "if my_accuracies:\n",
    "    # Plot it\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(my_n_estimators, my_accuracies, marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Trees')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Your Forest: Accuracy vs Number of Trees')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print values\n",
    "    for n, acc in zip(my_n_estimators, my_accuracies):\n",
    "        print(f\"n_estimators={n}: accuracy={acc:.4f}\")\n",
    "else:\n",
    "    print(\"TODO: Train your forests and record accuracies\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjD9n_qsfAP6"
   },
   "source": [
    "## Reflection Questions (TASK 3)\n",
    "\n",
    "**Discuss with your partner:**\n",
    "\n",
    "1. Did your forest beat your single tree? By how much?\n",
    "2. Where did you see diminishing returns? (How many trees before accuracy plateaued?)\n",
    "3. If you had to pick one for production, which would you use? Why?\n",
    "4. When might you *not* want to use a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dA0DI7dqfAP6"
   },
   "source": [
    "# TASK 3: Reflection\n",
    "# Type your thoughts as comments\n",
    "\n",
    "# 1. Forest vs Single Tree:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 2. Diminishing returns at:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 3. For production, I would choose:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 4. When NOT to use forests:\n",
    "#    YOUR ANSWER HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeziIqkbfAP6"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Single trees are unstable** → small data changes → very different trees\n",
    "2. **Forests reduce variance** → many trees averaged → stable, accurate predictions\n",
    "3. **More trees help, then plateau** → diminishing returns kick in (usually 50-100 trees is enough)\n",
    "4. **Forests are still interpretable** → feature importance tells you what matters\n",
    "5. **Ensemble is a fundamental ML principle** → applies to many algorithms, not just trees\n",
    "\n",
    "**Trade-off:** Forests are more accurate but less interpretable than single trees. You can't easily \"see\" how a forest decides, unlike a single tree.\n",
    "\n",
    "**Next steps:** XGBoost, Gradient Boosting—take this idea further with adaptive tree selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_type": "python",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
