{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBUqdFiHb1xp"
   },
   "source": [
    "# Decision Trees: Real-World Example\n",
    "## Customer Churn Prediction\n",
    "\n",
    "You already know how decision trees work conceptually. Today, we're seeing them in action on real data.\n",
    "\n",
    "**Goal:** Understand how trees make decisions by visualizing actual splits and manually tracing predictions through the tree.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnpuMeO1b1xr"
   },
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "We'll use pandas for data, scikit-learn for the tree, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dJHo0T0ub1xr"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make plots look nice\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzcXGFivb1xs"
   },
   "source": [
    "## Part 1: Load & Explore the Data\n",
    "\n",
    "We're using a real telecom customer churn dataset. The question: which customers are likely to leave (churn)?\n",
    "\n",
    "This matters because acquiring new customers is expensive. If a company can predict who's about to leave, they can offer retention deals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MP1EUq3Cb1xs"
   },
   "source": [
    "# Load the telecom churn dataset directly from GitHub\n",
    "url = 'https://raw.githubusercontent.com/TUHHStartupEngineers/dat_sci_ss20/master/13/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn names and types:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQp-hDIgb1xs"
   },
   "source": [
    "### Quick EDA: Understand the Target\n",
    "\n",
    "Let's see what we're predicting: Do customers churn or not?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jTiqnDzOb1xs"
   },
   "source": [
    "# Check the Churn column\n",
    "print(f\"Churn column unique values: {df['Churn'].unique()}\")\n",
    "print(f\"\\nChurn distribution:\")\n",
    "print(df['Churn'].value_counts())\n",
    "churn_rate = (df['Churn'] == 'Yes').mean()\n",
    "print(f\"\\nChurn rate: {churn_rate:.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3sP4OGXb1xt"
   },
   "source": [
    "### Prepare Data for the Tree\n",
    "\n",
    "Decision trees need numeric features. We'll:\n",
    "1. Drop columns we don't need (identifiers, etc.)\n",
    "2. Encode categorical features (like 'Contract') to numbers\n",
    "3. Convert target to binary (Yes=1, No=0)\n",
    "4. Split into train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ppn5SRP2b1xt"
   },
   "source": [
    "# Make a copy to avoid warnings\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Drop non-informative columns (customerID)\n",
    "df_clean = df_clean.drop(columns=['customerID'])\n",
    "\n",
    "# Convert Churn to binary (Yes=1, No=0)\n",
    "y = (df_clean['Churn'] == 'Yes').astype(int)\n",
    "X = df_clean.drop(columns=['Churn'])\n",
    "\n",
    "# Encode categorical columns to numeric\n",
    "le_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        le_dict[col] = le\n",
    "\n",
    "# Fix TotalCharges column (it has some spaces, convert to numeric)\n",
    "X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "X['TotalCharges'].fillna(X['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures we're using:\")\n",
    "print(X.columns.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e2QOq9b1xt"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Train & Visualize a Decision Tree\n",
    "\n",
    "Now we train a decision tree on the churn data. We'll keep it shallow (max_depth=5) so we can actually *see* and understand the splits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OJGXp7HUb1xt"
   },
   "source": [
    "# Train a decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Check accuracy\n",
    "train_acc = tree.score(X_train, y_train)\n",
    "test_acc = tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF16WE5Xb1xt"
   },
   "source": [
    "### Visualize the Tree\n",
    "\n",
    "Here's the actual tree. Read it top-to-bottom:\n",
    "- Each box is a **decision node**: a question like \"Is tenure <= 24.5?\"\n",
    "- The arrows point to what happens based on YES (left) or NO (right)\n",
    "- The **leaf nodes** (bottom) show: what class do we predict? How many samples? How many of each class?\n",
    "\n",
    "Try to trace one path from root to leaf. That's exactly how the tree makes a decision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M21U8VgAb1xt"
   },
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(\n",
    "    tree,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Stay', 'Churn'],\n",
    "    filled=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Decision Tree for Customer Churn (max_depth=5)', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read the tree:\")\n",
    "print(\"- Top: Root node (initial split)\")\n",
    "print(\"- YES (left arrow) or NO (right arrow)\")\n",
    "print(\"- Color: Darker green = more 'Stay', darker red = more 'Churn'\")\n",
    "print(\"- Leaf (bottom): Prediction class, samples in leaf, value [#Stay, #Churn]\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDt5d562b1xt"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Manually Trace a Prediction\n",
    "\n",
    "Let's pick one customer and trace them through the tree manually. This shows exactly how trees make decisions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KX0BjyINb1xt"
   },
   "source": [
    "# Pick one customer from the test set\n",
    "test_idx = 5\n",
    "customer = X_test.iloc[test_idx]\n",
    "actual_churn = y_test.iloc[test_idx]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"CUSTOMER #{test_idx}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCustomer profile (selected features):\")\n",
    "important_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Contract', 'InternetService']\n",
    "for col in important_features:\n",
    "    if col in X.columns:\n",
    "        print(f\"  {col}: {customer[col]:.2f}\")\n",
    "print(f\"\\nActual outcome: {'CHURNED' if actual_churn == 1 else 'STAYED'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6YkvNV2b1xt"
   },
   "source": [
    "### Trace the Path\n",
    "\n",
    "Now let's manually walk this customer through the tree, following each split. This is how the tree actually makes the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "najoPf5ab1xu"
   },
   "source": [
    "# Make a prediction and show the tree's reasoning\n",
    "prediction = tree.predict([customer])[0]\n",
    "prediction_proba = tree.predict_proba([customer])[0]\n",
    "\n",
    "print(\"\\nTREE'S DECISION PATH:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get decision path\n",
    "decision_path = tree.decision_path([customer]).toarray()[0]\n",
    "node_index = np.where(decision_path)[0]\n",
    "\n",
    "print(f\"\\nFollowing the path through the tree:\")\n",
    "step = 1\n",
    "for node_id in node_index:\n",
    "    feature = tree.tree_.feature[node_id]\n",
    "    threshold = tree.tree_.threshold[node_id]\n",
    "\n",
    "    if feature != -2:  # -2 means leaf node\n",
    "        feature_name = X.columns[feature]\n",
    "        customer_value = customer[feature_name]\n",
    "        direction = \"YES (<=)\" if customer_value <= threshold else \"NO (>)\"\n",
    "        print(f\"\\n  Step {step}: Is {feature_name} <= {threshold:.2f}?\")\n",
    "        print(f\"            {feature_name} = {customer_value:.2f}\")\n",
    "        print(f\"            Answer: {direction}\")\n",
    "        step += 1\n",
    "    else:\n",
    "        # Leaf node\n",
    "        value = tree.tree_.value[node_id][0]\n",
    "        stay_count = int(value[0])\n",
    "        churn_count = int(value[1])\n",
    "        print(f\"\\n  \\\"\\\"\\\" LEAF NODE (Decision Made!) \\\"\\\"\\\"\")\n",
    "        print(f\"  In this group: {stay_count} stayed, {churn_count} churned\")\n",
    "        print(f\"  Prediction: {'CHURN' if churn_count > stay_count else 'STAY'}\")\n",
    "        print(f\"  Confidence: {max(prediction_proba)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TREE PREDICTION: {'CHURNED' if prediction == 1 else 'STAYED'}\")\n",
    "print(f\"ACTUAL OUTCOME: {'CHURNED' if actual_churn == 1 else 'STAYED'}\")\n",
    "print(f\"✓ CORRECT!\" if prediction == actual_churn else \"✗ WRONG\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjIMB1DIb1xu"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Importance\n",
    "\n",
    "Which features does the tree rely on most when making decisions?\n",
    "\n",
    "The tree will learn to split on features that best separate churners from non-churners. If a feature appears high in the tree, it's important."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KzUsmM-Rb1xu"
   },
   "source": [
    "# Extract feature importance\n",
    "importances = tree.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 10):\")\n",
    "print(feature_importance_df.head(10).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zulj7QXb1xu"
   },
   "source": [
    "### Visualize Feature Importance\n",
    "\n",
    "Which features matter most for predicting churn? The tree learned this automatically!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4IqpPUkYb1xu"
   },
   "source": [
    "# Plot top 10 features\n",
    "top_n = 10\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Most Important Features for Predicting Churn')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe tree relies most on:\")\n",
    "for idx, (_, row) in enumerate(top_features.head(3).iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['feature']} ({row['importance']:.1%})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYAwa0B-b1xu"
   },
   "source": [
    "---\n",
    "\n",
    "## YOUR TURN: Pairs Practice (20 minutes)\n",
    "\n",
    "Now you build your own tree. Work in pairs.\n",
    "\n",
    "### Task 1: Train & Visualize Your Own Tree (10 min)\n",
    "\n",
    "- Train a decision tree with max_depth=5 on the same churn data\n",
    "- Visualize it\n",
    "- **Compare to the demo tree above:** Are the splits different? If so, why?\n",
    "\n",
    "### Task 2: Reflect (5 min)\n",
    "\n",
    "- **What surprised you about the tree's splits?**\n",
    "- **If you were a telecom company using this tree, what would you do?** (Who would you target for retention offers?)\n",
    "\n",
    "### Task 3: Debug & Ask Questions (5 min)\n",
    "\n",
    "Stuck? Ask questions. This is the time to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgdj4vIJb1xu"
   },
   "source": [
    "---\n",
    "\n",
    "## Student Practice: Code Along Below\n",
    "\n",
    "Use the cells below to complete the tasks. Start fresh if you want, or modify the demo code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CJkSjqThb1xu"
   },
   "source": [
    "# TASK 1a: Train your own tree (use max_depth=5, same as demo)\n",
    "# TODO: Fit a DecisionTreeClassifier on X_train and y_train\n",
    "# Hint: Use DecisionTreeClassifier(max_depth=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "my_tree = None  # REPLACE THIS with your tree\n",
    "\n",
    "if my_tree:\n",
    "    my_train_acc = my_tree.score(X_train, y_train)\n",
    "    my_test_acc = my_tree.score(X_test, y_test)\n",
    "    print(f\"Your training accuracy: {my_train_acc:.3f}\")\n",
    "    print(f\"Your test accuracy: {my_test_acc:.3f}\")\n",
    "else:\n",
    "    print(\"TODO: Train your tree in this cell!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FJ8sl-zGb1xu"
   },
   "source": [
    "# TASK 1b: Visualize your tree\n",
    "# TODO: Create a plot_tree visualization of my_tree\n",
    "# Hint: Copy the visualization code from Part 2, but use my_tree instead of tree\n",
    "\n",
    "if my_tree:\n",
    "    plt.figure(figsize=(25, 12))\n",
    "    # YOUR CODE HERE\n",
    "    # plot_tree(my_tree, ...)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train your tree first (cell above)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQ2-G1Bqb1xu"
   },
   "source": [
    "# TASK 1c: Extract and plot feature importance from YOUR tree\n",
    "# TODO: Get feature_importances_ from my_tree and visualize\n",
    "\n",
    "if my_tree:\n",
    "    my_importances = my_tree.feature_importances_\n",
    "    my_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': my_importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Plot top 10 features\n",
    "    top_features = my_importance_df.head(10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='darkgreen')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Your Tree: Top 10 Most Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train your tree first\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26vLpL7Eb1xu"
   },
   "source": [
    "## Reflection Questions (TASK 2)\n",
    "\n",
    "**Discuss these with your pair partner. Write answers as comments below:**\n",
    "\n",
    "1. How does your tree compare to the demo tree? Are the top splits different?\n",
    "2. What surprised you about which features are important?\n",
    "3. If you were the telecom CEO, how would you use this tree to reduce churn?\n",
    "4. What could go wrong if you relied on this tree blindly?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DK2I5O9xb1xu"
   },
   "source": [
    "# TASK 2: Reflection\n",
    "# Type your thoughts below as comments\n",
    "\n",
    "# 1. Comparison to demo tree:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 2. Surprising features:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 3. As a CEO, I would:\n",
    "#    YOUR ANSWER HERE\n",
    "\n",
    "# 4. What could go wrong:\n",
    "#    YOUR ANSWER HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7wFVh_Sb1xu"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key takeaways from this notebook:**\n",
    "\n",
    "1. **Trees are interpretable**: You can see *exactly* how they decide—no black boxes\n",
    "2. **Manual tracing matters**: Walking through splits shows how a tree thinks about one customer\n",
    "3. **Feature importance is useful**: But it's not the whole story—you need context\n",
    "4. **Trees are unstable**: Your tree's splits might differ from the demo tree. Small data changes → different trees. This is a key limitation.\n",
    "\n",
    "**This is where Random Forests come in.** By training many trees and averaging, we get:\n",
    "- ✓ Better accuracy (reduces overfitting)\n",
    "- ✓ More stable predictions\n",
    "- ✓ Still interpretable (via feature importance)\n",
    "\n",
    "Next: Random forests address tree instability and show why ensembles are powerful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_type": "python",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
